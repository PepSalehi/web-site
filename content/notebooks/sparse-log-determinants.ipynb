{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Monte Carlo Estimates of Log-Determinants"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In statistics, we often need to compute a log-determinant as part of a multivariate maximum likelihood problem for non-independent data. The matrix for which we may need a log-determinant can be very large, though more often than not it is also very sparse. Direct methods of computing the determinant for a symmetric variance-covariance matrix often use the Cholesky decomposition or in certain cases an LU decomposition can be used.\n",
      "\n",
      "I have recently need to tackle this problem for the estimation of some spatial-econometric models. One good thing about the variance-covariance in spatial econometric models is that they're sparse. Unfortunately, all of the sparse Cholesky routines that I am aware of are not suitable to be included in BSD-compatible projects like [statsmodels](http://statsmodels.sourceforge.net) and [pysal](http://pythonhosted.org/PySAL/). Furthermore, sparse methods (sometimes) rely on particular orderings of the matrices to achieve good performance; however, these orderings are not always possible to achieve with spatial weighting matrices. Another good thing about working with spatial econometric models is that we often don't need exact estimates. Approximations often do just fine. This led me to a 1999 paper by Barry and Pace in which they introduce a simple Monte Carlo method to approximate log-determinants of matrices, \"A Monte Carlo Estimator of Log Determinants of Large Sparse Matrices.\" [[pdf](http://www.spatial-statistics.com/pace_manuscripts/Lin_algebra_appl/lin_algebra_applications.pd])]. See also Chapter 4.4 in LeSage and Pace's Introduction to Spatial Econometrics.\n",
      "\n",
      "To describe the problem in a little more detail. We need to calculate $\\ln|I_n - \\rho W|$ where $I_n$ is an $n\\times n$ identity matrix, $\\rho$ is a scalar in $(-1,1)$, and $W$ has been rescaled such that its eigenvalues are in $(-1,1)$. Direct methods would require the computation of this log determinant for a number of different values of $\\rho$. However, we can develop an MC approximation that avoids these recomputations by first noting that\n",
      "\n",
      "$$\\ln|I_n - \\rho W| = \\operatorname{tr}(\\ln(I_n - \\rho W))$$\n",
      "\n",
      "where $\\operatorname{tr}$ denotes the trace of the matrix. Next note that the matrix logarithm has the power series expansion\n",
      "\n",
      "$$\\ln(I_n - \\rho W) = -\\sum_{i=1}^{\\infty}\\frac{\\rho^i W^i}{i}$$\n",
      "\n",
      "Since the trace is a linear operation, we can write\n",
      "\n",
      "$$\\ln|I_n - \\rho W| = -\\sum_{i=1}^{\\infty}\\frac{\\rho^i \\operatorname{tr}(W^i)}{i}$$\n",
      "\n",
      "Note that this will also hold for non-symmetric W with complex eigenvalues or for complex $\\rho$. Assuming that truncation error is small, we can write a finite approximation of the above power series\n",
      "\n",
      "$$\\ln|I_n - \\rho W| = -\\sum_{i=1}^{m}\\frac{\\rho^i \\operatorname{tr}(W^i)}{i}$$\n",
      "\n",
      "The computation for $\\operatorname{tr}(W^i)$ can be computationally costly for larger values of $i$. Barry and Pace introduce the following \"trick\" to calculate the trace. Let the vector $\\boldsymbol{x}$ come from a $N_n(0,I)$ distribution and assume we have some matrix $D$. Since the square of any single $x_i\\sim N(\\cdot)$ gives $x_i^2\\sim\\chi^2_1$ with $\\mathbb{E}[x_i^2]=1$ and $\\mathbb{E}[x_ix_j]=0$ by construction, then given the quadratic form $\\boldsymbol{x}^\\prime D\\boldsymbol{x}$ we have the following using $n=2$ for illustration\n",
      "\n",
      "$$\\begin{align}\\boldsymbol{x}^\\prime D\\boldsymbol{x} &=x_1^2d_{1,1} + x_2^2d_{2,2} + x_1x_2(d_{1,2} + d_{2,1}) \\cr\n",
      "\\mathbb{E}[\\boldsymbol{x}^\\prime D\\boldsymbol{x}] &=\\mathbb{E}[x_1^2d_{1,1} + x_2^2d_{2,2} + x_1x_2(d_{1,2} + d_{2,1})] \\cr\n",
      "&= d_{1,1} + d_{2,2} \\cr\n",
      "&= \\operatorname{tr(D)}\n",
      "\\end{align}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This result suggests the following algorithm, which I'll implement below.\n",
      "\n",
      "     pick x_i = randn(n)\n",
      "     set z = x_i\n",
      "     for t = 1:m\n",
      "         z = Wz\n",
      "         tr(W^i) = n*(x_i'z)/(t*x_i'x_i)\n",
      "         \n",
      "Storing all of the estimates for $\\operatorname{tr}(W^i)$, we can use them to compute our truncated power series. \n",
      "\n",
      "$$a^{\\prime}b$$\n",
      "\n",
      "where $a$ are the estimates of the trace and $b=[\\rho, \\rho^2, \\dots, \\rho^m]$. It's common to compute the estimate for a grid of values for $\\rho$ so that the log determinant over all possible values for $\\rho$ can be computed in a vectorized way and then to use interpolation to get intermediate values as needed.\n",
      "\n",
      "Notice that I used $x_i$ for the random vector above. This suggests that we can do this numerous times and take the average to get a better estimate for the log determinant. Now let's look at the implementation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.set_printoptions(suppress=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take an example of a contiguity matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = np.zeros((7,7), dtype=float)\n",
      "W[0,1] = 1.\n",
      "W[-1, -2] = 1.\n",
      "for i in range(5):\n",
      "    W[i+1,i] = .5\n",
      "    W[i+1, i+2] = .5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "array([[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
        "       [ 0.5,  0. ,  0.5,  0. ,  0. ,  0. ,  0. ],\n",
        "       [ 0. ,  0.5,  0. ,  0.5,  0. ,  0. ,  0. ],\n",
        "       [ 0. ,  0. ,  0.5,  0. ,  0.5,  0. ,  0. ],\n",
        "       [ 0. ,  0. ,  0. ,  0.5,  0. ,  0.5,  0. ],\n",
        "       [ 0. ,  0. ,  0. ,  0. ,  0.5,  0. ,  0.5],\n",
        "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ]])"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is what is called a row-stochastic weight matrix in the spatial regression literature. That is, the rows all sum to 1. This guarantees that the eigenvalues are constrained to be from -1 to 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.eig(W)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "array([-1.       , -0.8660254, -0.5      , -0.       ,  0.5      ,\n",
        "        1.       ,  0.8660254])"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rho = .25"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Direct computation of $\\ln|I_n - \\rho W|$ yields"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.log(np.linalg.det(np.eye(W.shape[0]) - rho*W))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "-0.12829609729207081"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or better yet, use the more numerically stable `slogdet`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.slogdet(np.eye(W.shape[0]) - rho*W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "(1.0, -0.12829609729207084)"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below is the implementation of the algorithm of Barry and Pace. I haven't done any timings or optimizations for it. I first need to see how it performs in practice. Notice also that it only takes a single $\\rho$. This is easily updated to work with a grid for $\\rho$, though I'm thinking about using a memoization pattern for the trace estimates in my own code, depending on speed, so I've left it as is here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One addition to the algorithm above is given via the `exact` keyword. If $W$ is symmetric with zeros on the diagonals, which it always will be for spatial econometric applications, then we can compute the first two moments exactly at small cost. $\\operatorname{tr}(W=0)$ and $\\operatorname{tr}(W^2)=\\sum W^2$ where the square is taken elementwise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy import sparse\n",
      "\n",
      "def logdet_mc(W, rho=1, seed=None, nterms=10, nrepl=100, exact=True):\n",
      "    \"\"\"\n",
      "    Compute the approximate log-determinant of I - rho*W.\n",
      "    \n",
      "    W : array or sparse\n",
      "        An n x n array or a scipy.sparse matrix.\n",
      "    rho : scalar\n",
      "        The autoregressive coefficient.\n",
      "    seed : int or None\n",
      "        Seed passed to numpy.random.seed.\n",
      "    nterms : int\n",
      "        The number of terms for the power series estimate of the log determinate.\n",
      "    nrepl : int\n",
      "        The number of power series to estimate. The log determinate is the average\n",
      "        of each these.\n",
      "    exact : bool\n",
      "        Use exact if W is symmetric with zeros on the diagonal to use the exact \n",
      "        first two moments.\n",
      "    \"\"\"\n",
      "    if seed is not None:\n",
      "        np.random.seed(seed)       \n",
      "\n",
      "    if hasattr(W, 'dot'): # then prefer it (for sparse) and newer numpy\n",
      "        Wdot = W.dot\n",
      "    else:\n",
      "        Wdot = lambda x : np.dot(W, x)\n",
      "        \n",
      "    nobs = W.shape[0]\n",
      "    try:\n",
      "        assert W.shape[1] == nobs\n",
      "    except AssertionError:\n",
      "        raise ValueError(\"W is not square.\")\n",
      "    \n",
      "    allT = np.empty((nrepl, nterms))\n",
      "    \n",
      "    range_ = np.arange(1., nterms+1)    \n",
      "    \n",
      "    for j in range(nrepl):\n",
      "\n",
      "        T = np.empty(nterms)\n",
      "        # get the \"seed\"\n",
      "        u1 = np.random.randn(nobs, 1)\n",
      "        z_t = u1\n",
      "        uTu = np.dot(u1.T, u1).item() # scalar\n",
      "    \n",
      "        # compute the terms in the expansion\n",
      "        for i in range(1, nterms+1):\n",
      "            z_t = Wdot(z_t)\n",
      "            T[i-1] = np.dot(u1.T, z_t).item()\n",
      "        \n",
      "        T *= nobs/(range_*uTu)\n",
      "        allT[j] = T\n",
      "    \n",
      "    \n",
      "    if exact: # must be symmetric and zeros on diagonal\n",
      "        if sparse.issparse(W):\n",
      "            exactT = [0, np.sum(W.data**2)/2.]\n",
      "        else:\n",
      "            exactT = [0, np.sum(W**2)/2.]\n",
      "        \n",
      "        allT[:,:2] = exactT\n",
      "    \n",
      "    meanT = np.mean(allT, axis=0)\n",
      "    \n",
      "    b = -rho ** range_\n",
      "\n",
      "    return np.dot(b, meanT)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rho = .75"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exact estimate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.slogdet(np.eye(W.shape[0]) - rho * W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 60,
       "text": [
        "(1.0, -1.5261936420271165)"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Monte-carlo estimate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logdet_mc(W, rho, seed=12, nterms=10, exact=True, nrepl=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "-1.6339584180549775"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimate using [CHOLMOD](http://www.cise.ufl.edu/research/sparse/cholmod/) from [scikits.sparse](https://github.com/njsmith/scikits-sparse)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sparseW = sparse.csc_matrix(W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scikits.sparse import cholmod"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = cholmod.cholesky((sparse.eye(7, format=\"csc\") - rho*sparseW))\n",
      "f.logdet()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "-2.0152902872928142"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Pace and Barry Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following example uses the example data from Pace and Barry (1997) on 3,107 US Counties for the 1980 Presidential election. You can find the data and more information [here](http://spatial-econometrics.com/data/contents.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from scipy import sparse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dta = pd.read_table(\"http://spatial-econometrics.com/data/elect.dat\", sep=\" *\", header=None)[[4,5]]\n",
      "dta.rename(columns={4 : \"lat\", 5 : \"long\"}, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the 5 nearest neighbors using Euclidean distance. Use 5 because it counts itself and we want the 4 nearest neighbors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import neighbors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "neigh = neighbors.NearestNeighbors(n_neighbors=5)\n",
      "neigh.fit(dta.values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "NearestNeighbors(algorithm='auto', leaf_size=30, n_neighbors=5, p=2,\n",
        "         radius=1.0)"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nobs = len(dta)\n",
      "W = sparse.lil_matrix((nobs, nobs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, row in dta.iterrows():\n",
      "    closest = neigh.kneighbors(row.values, return_distance=False)\n",
      "    for j in closest[0]:\n",
      "        if j == i: # don't count own observation\n",
      "            continue\n",
      "        W[i, j] = 1."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Make row stochastic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = W / 4.\n",
      "assert all(W.sum(1) == 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert to CSC format for calculations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W = W.tocsc()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Set $\\rho$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rho = .1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate the exact log determinant."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.linalg.slogdet(np.eye(W.shape[0]) - rho*W.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 74,
       "text": [
        "(1.0, -3.346557139406221)"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the monte carlo approximation of the log determinant."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "logdet_mc(W, rho)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "-3.9456500690606093"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute the log determinant using CHOLMOD and scikits.sparse again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = cholmod.cholesky(sparse.eye(W.shape[0], format=\"csc\") - rho * W)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f.logdet()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "-3.9400025132708647"
       ]
      }
     ],
     "prompt_number": 77
    }
   ],
   "metadata": {}
  }
 ]
}